# 🌐 Wiki Chatbot Web デプロイメント完全ガイド

**🎯 対象読者**: プログラミング完全初心者〜上級者まで誰でも対応
**⏱️ 所要時間**: 15分（最速デプロイ）〜60分（本格運用設定）
**💻 前提条件**: パソコンとインターネット接続のみ（特別なソフト不要）
**💰 費用**: 完全無料〜月数百円（使用量による）

> 🚀 **このガイドの超強力な特徴**
> - **🔰 完全初心者フレンドリー**: コーディング経験ゼロでもOK！画面の通りに進めるだけ
> - **📋 コピペで完結**: コードは全て用意済み。コピー＆ペーストだけで動きます
> - **🛡️ データ完全保護**: アップロードファイル・チャット履歴が絶対に消えない永続化システム
> - **🆓 完全無料スタート**: 最初は1円もかからずに始められます
> - **📱 スマホ対応**: パソコンでもスマホでも使える完全レスポンシブ
> - **🔒 企業レベルセキュリティ**: パスワード認証・セッション管理・不正アクセス防止
> - **🔧 ぴよぴよ初心者サポート**: 「これ何？」を全て解説。専門用語は使いません！

## 🎉 このガイドで作れるもの

✅ **世界中からアクセス可能**な自分専用チャットボット
✅ **PDF・Wordファイルの内容**について質問できるAIアシスタント
✅ **データ永続化**でファイルが絶対に消えない安心システム
✅ **管理画面付き**で利用状況も丁寧に分析
✅ **パスワード保護**で不正利用を完全ブロック

## 🌈 技術知識ゼロでも大丈夫！

> 🤗 **「プログラミングなんて全然わからない...」という方へ**
> このガイドは「コンピューターってどうやって動くの？」レベルの方でも理解できるよう書かれています。
> 難しい専門用語は一切使わず、全てを日常語で説明します。
> コピペするだけで動くので、コードを理解する必要もありません！

---

## 📋 目次

### 🔰 **STEP 1: 超初心者向け（まずはここから！）**
1. [🎬 5分で理解！全体の流れ](#1-5分で理解全体の流れ)
2. [🔑 AIとお話するためのパスワード取得](#2-aiとお話するためのパスワード取得)
3. [📁 自分のファイル置き場を作ろう](#3-自分のファイル置き場を作ろう)
4. [🌐 世界に公開！あなたのチャットボット](#4-世界に公開あなたのチャットボット)

### 💾 **STEP 2: データを絶対に消さない設定（超重要！）**
5. [🛡️ アップロードファイルを永遠に保護](#5-アップロードファイルを永遠に保護)
6. [💬 会話履歴を永久保存](#6-会話履歴を永久保存)
7. [🔐 悪い人から守るセキュリティ](#7-悪い人から守るセキュリティ)

### 🏢 **STEP 3: 会社でも使える本格設定（中級者向け）**
8. [☁️ 大きな会社向けクラウド設定](#8-大きな会社向けクラウド設定)
9. [⚡ スピードアップ＆節約術](#9-スピードアップ節約術)
10. [🆘 困った時の解決方法](#10-困った時の解決方法)

### 📚 **STEP 4: もっと詳しく知りたい人向け**
11. [🤓 技術的な詳細説明](#11-技術的な詳細説明)
12. [🔧 カスタマイズ方法](#12-カスタマイズ方法)
13. [📈 運用・保守のベストプラクティス](#13-運用保守のベストプラクティス)

---

## 1. 🎬 5分で理解！全体の流れ

### 🌟 「これから何をするの？」を優しく説明

初心者の方でも迷わないよう、まずは **「今から何をするのか」** を分かりやすく説明します！

#### 🏠 家を建てるのと同じ！チャットボット作成の流れ

```
🏗️ あなたのチャットボットを作る手順（家を建てるのと同じ！）

1. 🔑 材料の準備     → AIサービスを使うためのパスワードをもらう
   （建材を買う）      （OpenAIという会社からパスワードをもらいます）

2. 📁 土地の準備     → あなたのプログラムを置く場所を借りる
   （土地を借りる）    （GitHubという無料サービスを使います）

3. 🏠 家の建設      → インターネットで誰でも使えるようにする
   （家を建てる）      （Streamlit Cloudという無料サービスを使います）

4. 🛡️ セキュリティ   → 悪い人が勝手に使えないように鍵をかける
   （セキュリティ）    （パスワードで保護します）

5. 💾 引っ越し準備   → データが消えないように永久保存設定
   （家具を運ぶ）     （大切なファイルを絶対に消えないようにします）
```

### 🎯 **完成すると何ができる？**

✅ **📱 スマホからでも使える**: iPhone、Android、パソコンどれでもOK！
✅ **🌍 世界中どこからでも**: インターネットがあれば地球の裏側からでもアクセス
✅ **📄 PDF質問マシン**: アップロードしたPDF・Wordファイルについて何でも質問
✅ **🧠 AIが回答**: ChatGPTと同じ技術で的確に答えてくれます
✅ **🔐 安全・安心**: パスワード保護で関係者以外は使用不可
✅ **💾 データ永久保存**: 一度アップロードしたファイルは絶対に消えません

### ⏰ **実際にかかる時間**

| 手順 | 説明 | 所要時間 | 難易度 |
|------|------|----------|--------|
| 🔑 パスワード取得 | OpenAIのサイトでアカウント作成 | **3分** | ⭐☆☆（超簡単） |
| 📁 置き場所準備 | GitHubでファイル置き場を作成 | **5分** | ⭐☆☆（超簡単） |
| 🌐 Web公開 | Streamlit Cloudでサイト公開 | **7分** | ⭐⭐☆（簡単） |
| 🛡️ セキュリティ設定 | パスワード・暗号化設定 | **5分** | ⭐⭐☆（簡単） |
| 💾 永続化設定 | データ消失防止設定 | **10分** | ⭐⭐⭐（少し注意） |

**合計: 約30分** （コーヒーを1杯飲む時間で完成！）

### 💰 **料金はいくら？**

> 🆓 **最初は完全無料でスタート可能！**

| サービス | 料金 | 何をしてくれる？ |
|----------|------|----------------|
| **GitHub** | **完全無料** | あなたのプログラムを保管 |
| **Streamlit Cloud** | **完全無料** | Webサイトとして公開 |
| **OpenAI API** | **月$3〜10程度** | AIが回答を作成 |

**月額300円〜1,000円程度** で本格的なAIチャットボットが運用できます！
（缶コーヒー3本分の値段です）

### 🚨 **絶対に読んで！超重要な注意点**

> ⚠️ **「データ永続化」は必ず設定してください！**
>
> これを設定しないと、以下のような **悲惨なこと** が起こります：
> - 📄 アップロードしたPDFファイルが **全部消える**
> - 💬 今までのチャット履歴が **全部消える**
> - 🔄 アプリが再起動するたびに **最初からやり直し**
> - 😭 せっかくの苦労が **水の泡**
>
> でも大丈夫！このガイドの通りに進めれば、**絶対に消えない設定** になります！

---

### 🎪 **いよいよ始めましょう！**

準備はできましたか？それでは、一緒に **あなた専用のAIチャットボット** を作っていきましょう！

初心者の方も、「1つずつゆっくり」進めれば大丈夫です。
分からないことがあっても、このガイドに **全て答え** が書いてあります！

🚀 **次のSTEP**: [🔑 AIとお話するためのパスワード取得](#2-aiとお話するためのパスワード取得)

---

## 2. 🔑 AIとお話するためのパスワード取得

### 🤔 **「APIキー」って何？**

> 💡 **超簡単に説明！**
>
> APIキー = **AIサービスを使うための会員証** だと思ってください！
>
> - 📱 スマホアプリを使うのに「アカウント」が必要なのと同じ
> - 🏪 お店で買い物するのに「会員カード」が必要なのと同じ
> - 🚗 車を運転するのに「運転免許証」が必要なのと同じ
>
> つまり、**「私はちゃんとした利用者ですよ」** ということをAIに証明するための **身分証明書** です！

### 🏆 **OpenAI（ChatGPTの会社）がなぜ最高？**

OpenAIを選ぶべき理由を、初心者向けに分かりやすく説明します：

| 🌟 理由 | 🔰 初心者にとってのメリット |
|---------|---------------------------|
| **🗣️ 日本語が超得意** | 質問も回答も自然な日本語でスムーズ |
| **🧠 とても賢い** | 的確で詳しい回答をしてくれる |
| **📚 情報が豊富** | 困った時の解決方法がネットにたくさん |
| **💰 料金が分かりやすい** | 使った分だけの明確な課金システム |
| **🛡️ 安全・安心** | 世界中で使われている信頼できるサービス |

### 📝 **取得手順（画面の写真付きで詳しく解説！）**

#### **STEP 1: OpenAI公式サイトにアクセス（30秒）**

1. **以下のリンクをクリック**してOpenAI公式サイトを開いてください：
   ```
   👆 クリック → https://platform.openai.com/api-keys
   ```

2. **画面に表示される内容**：
   - 英語のサイトですが、心配しないでください！
   - 「Sign up」（アカウント作成）ボタンが見つかります

#### **STEP 2: アカウント作成（2分）**

3. **「Sign up」ボタンをクリック**
   - 新規アカウント作成画面に移動します

4. **必要な情報を入力**：
   ```
   📧 Email address（メールアドレス）
   ┗━ 普段使っているメールアドレスを入力

   🔒 Password（パスワード）
   ┗━ 8文字以上の安全なパスワードを入力
       （例：mypassword123）

   👤 First name & Last name（名前）
   ┗━ 本名でなくてもOK（例：Taro Yamada）
   ```

5. **メール認証を完了**：
   - 入力したメールアドレスに確認メールが届きます
   - メール内の「Verify email address」をクリック
   - これでアカウント作成完了！

#### **STEP 3: APIキー作成（2分）**

6. **APIキー作成画面に移動**：
   - ログイン後、左側のメニューから **「API keys」** をクリック

7. **新しいキーを作成**：
   - **「Create new secret key」** ボタンをクリック
   - 名前を聞かれたら **「WikiChatbot」** と入力
   - **「Create secret key」** をクリック

8. **🚨 超重要！キーをコピーして保存**：
   ```
   ⚠️ 画面に「sk-」で始まる長い文字列が表示されます

   例：sk-abcd1234efgh5678ijkl9012mnop3456qrst7890uvwx

   この文字列を必ずコピーして、メモ帳などに保存してください！

   ⚠️ この画面を閉じると二度と表示されません！
   ```

#### **STEP 4: 料金設定（1分）**

9. **安全のため料金上限を設定**：
   - 左メニューの **「Settings」** → **「Billing」** をクリック
   - **「Usage limits」** をクリック
   - **「Monthly budget」** に **「20」** と入力（月20ドル = 約3,000円の上限設定）
   - これで使いすぎて高額請求される心配がありません！

### 💰 **料金について（初心者向け詳細説明）**

#### **🔍 実際いくらかかるの？**

```
📊 リアルな使用例と料金

🏠 個人利用（少なめ）
├─ 1日10回質問 × 30日 = 月300回
├─ 使用モデル：GPT-3.5-turbo（安い方）
└─ 月額料金：約$3-5（450円-750円）☕️コーヒー2杯分

🏢 小規模チーム（普通）
├─ 1日50回質問 × 30日 = 月1,500回
├─ 使用モデル：GPT-3.5-turbo
└─ 月額料金：約$10-15（1,500円-2,250円）🍱お弁当2個分

🏭 本格運用（多め）
├─ 1日200回質問 × 30日 = 月6,000回
├─ 使用モデル：GPT-4o（高性能版）
└─ 月額料金：約$30-50（4,500円-7,500円）📚本数冊分
```

#### **💡 料金を抑える超簡単な方法**

| 🎯 節約方法 | 💰 節約効果 | 🔰 初心者でも簡単度 |
|-------------|-------------|-------------------|
| **GPT-3.5-turboを使う** | **90%節約** | ⭐⭐⭐ (設定1回だけ) |
| **質問を簡潔にする** | **50%節約** | ⭐⭐⭐ (意識するだけ) |
| **回答の長さを制限** | **30%節約** | ⭐⭐☆ (設定で簡単) |
| **不要な質問を避ける** | **20%節約** | ⭐⭐⭐ (当然のこと) |

> 📋 **初心者におすすめの設定**
> - モデル：**GPT-3.5-turbo**（十分高性能で安い）
> - 月額上限：**$20**（約3,000円）
> - これで月1,000〜2,000回の質問が可能！

### 🆘 **困った時のQ&A**

#### **❓ よくある質問**

**Q1: 英語のサイトで不安です...**
A1: 大丈夫！このガイドの通りに進めれば、英語が読めなくても確実にできます。重要な部分は全て日本語で説明しています。

**Q2: クレジットカードの登録が怖いです**
A2: OpenAIは世界中で使われている安全なサービスです。さらに上限設定をすることで、予想外の高額請求を防げます。

**Q3: APIキーを無くしてしまいました**
A3: 大丈夫！いつでも新しく作り直せます。古いキーは削除して、新しいキーを作成してください。

**Q4: 本当に月数百円で使えますか？**
A4: はい！個人利用なら月500円以下で十分使えます。上限設定もできるので安心です。

### ✅ **完了チェックリスト**

以下が全て完了したら次に進んでください：

- [ ] OpenAIアカウントを作成した
- [ ] メール認証を完了した
- [ ] APIキーを作成し、安全な場所に保存した
- [ ] 月額料金の上限を設定した（推奨：$20）
- [ ] APIキーが「sk-」で始まることを確認した

🎉 **お疲れ様でした！** これでAIとお話しする準備が整いました！

🚀 **次のSTEP**: [📁 自分のファイル置き場を作ろう](#3-自分のファイル置き場を作ろう)

---

## 3. 📁 GitHub準備（画面操作のみ）

### 3.1 GitHubアカウント作成

**既にお持ちの方はスキップしてください**

1. **GitHub公式サイト**にアクセス
   ```
   https://github.com/
   ```

2. **Sign up**をクリックしてアカウント作成
   - ユーザー名：半角英数字（例：`taro-yamada-2024`）
   - メール：普段使用するメールアドレス
   - パスワード：8文字以上の安全なパスワード

### 3.2 リポジトリ作成・アップロード

**方法1: GitHub Desktop使用（初心者推奨）**

1. **GitHub Desktop**をダウンロード・インストール
   ```
   https://desktop.github.com/
   ```

2. **リポジトリを作成**
   - GitHub Desktop起動
   - 「Create a new repository」選択
   - Name: `wiki-chatbot`
   - Local path: Wiki Chatbotフォルダを選択
   - 「Create repository」をクリック

3. **ファイルをアップロード**
   - 「Publish repository」をクリック
   - 「Keep this code private」のチェックを外す（無料プランの場合）
   - 「Publish repository」で完了

**方法2: ブラウザで直接アップロード（超初心者向け）**

1. **新規リポジトリ作成**
   - GitHubログイン後、右上の「+」→「New repository」
   - Repository name: `wiki-chatbot`
   - Public を選択
   - 「Create repository」をクリック

2. **ファイルをドラッグ&ドロップ**
   - 「uploading an existing file」をクリック
   - Wiki Chatbotフォルダの全ファイルをドラッグ&ドロップ
   - Commit message: `Initial upload`
   - 「Commit changes」をクリック

---

## 4. 🌐 無料Web公開

### 4.1 Streamlit Cloud セットアップ

1. **Streamlit Cloud**にアクセス
   ```
   https://share.streamlit.io/
   ```

2. **GitHubでログイン**
   - 「Continue with GitHub」をクリック
   - GitHub認証を完了

3. **アプリをデプロイ**
   - 「New app」をクリック
   - Repository: 先ほど作成した`wiki-chatbot`を選択
   - Branch: `main`
   - Main file path: `app.py`
   - 「Deploy!」をクリック

### 4.2 API Key設定（重要！）

**デプロイ後、アプリがエラーになった場合の対処法**

1. **アプリ設定画面にアクセス**
   - デプロイされたアプリ画面右上の「⚙️」をクリック
   - 「Settings」を選択

2. **Secrets設定**
   - 左メニューから「Secrets」を選択
   - 以下をコピペして貼り付け：

```toml
# あなたのOpenAI API Keyに置き換えてください
OPENAI_API_KEY = "sk-ここにあなたのAPIキーを貼り付け"

# パスワード設定（お好きなパスワードに変更）
WEB_PASSWORD = "mypassword123"

# 基本設定（そのままでOK）
MAX_QUERIES_PER_SESSION = 100
SESSION_TIMEOUT_HOURS = 24
ENABLE_FILE_UPLOAD = true
MAX_FILE_SIZE_MB = 10
```

3. **設定保存**
   - 「Save」をクリック
   - アプリが自動的に再起動されます

### 4.3 デプロイ成功確認

✅ **成功時の画面**
- ログイン画面が表示される
- パスワード入力でチャットボット画面に遷移
- ファイルアップロード機能が動作

❌ **失敗時の対処**
- エラーメッセージを確認
- [トラブルシューティング](#10-トラブルシューティング)を参照

---

## 5. 🛡️ アップロードファイルを永遠に保護

### 5.1 📚 データ永続化の重要性を完全理解

#### 🚨 「なんで私のファイルが消えちゃうの？」問題

**Streamlit Cloudの仕組み**

```
😱 問題発生のパターン
┌─────────────────────────────────────┐
│ 1. あなたがPDFファイルをアップロード    │ 📄
│ 2. AIがファイル内容を学習・記憶       │ 🧠
│ 3. 翌日アクセスすると...              │ 😴
│ 4. 「ファイルが見つかりません」エラー  │ ❌
└─────────────────────────────────────┘

なぜこうなる？
🏠 Streamlit Cloud = 「一時的な作業場所」
   - アプリが使われないと「お掃除」が入る
   - アップロードファイルは「ゴミ」として削除
   - データベースも全部リセット
```

#### ✅ 永続化で解決する問題

**1. ファイル消失問題の完全解決**
```
🔒 永続化前 vs 永続化後

【永続化前】😰
❌ 1日後: ファイル消失
❌ アプリ再起動: データ全消去
❌ 使うたび: 再アップロード必要
❌ 学習内容: 毎回ゼロからスタート

【永続化後】😍
✅ 何日後: ファイル永久保存
✅ アプリ再起動: データそのまま
✅ 使うたび: すぐに質問開始
✅ 学習内容: 蓄積されて賢くなる
```

**2. 企業利用での安心感**
```
📊 ビジネス利用での効果

🏢 会社の重要文書を安心アップロード
   ├─ 契約書、マニュアル、FAQ
   ├─ 一度設定したら永久利用
   └─ 新入社員教育にも活用

👥 チーム共有での活用
   ├─ 部署の知識ベース構築
   ├─ プロジェクト資料の共有
   └─ ナレッジマネジメント
```

### 5.2 🛠️ RAGデータベース永続化システム（技術詳細）

#### 5.2.1 永続化アーキテクチャ全体図

```
🏗️ データ永続化の全体構造

┌─────────────────────────────────────────────────────────┐
│                    🌐 Streamlit Cloud                     │
│  ┌─────────────────┐      ┌─────────────────────────────┐ │
│  │    📱 Web UI     │      │     🧠 RAG Processing       │ │
│  │                 │      │                             │ │
│  │ • ファイルアップロード │ ────▶ │ • 文書ベクトル化          │ │
│  │ • チャット画面     │      │ • ChromaDB知識ベース     │ │
│  │ • 管理画面       │      │ • 類似文書検索           │ │
│  └─────────────────┘      └─────────────────────────────┘ │
│           │                              │               │
│           ▼                              ▼               │
│  ┌─────────────────────────────────────────────────────┐   │
│  │            📦 永続化レイヤー                          │   │
│  │                                                     │   │
│  │ ├─ 🗃️ ファイルストレージ (data/products/)            │   │
│  │ │   ├─ PDF, Word, Excel原本                         │   │
│  │ │   ├─ メタデータ (.meta files)                     │   │
│  │ │   └─ ベクトルインデックス (.chroma/)               │   │
│  │                                                     │   │
│  │ ├─ 💾 SQLiteデータベース (data/chatbot.db)           │   │
│  │ │   ├─ チャット履歴テーブル                          │   │
│  │ │   ├─ ユーザーフィードバック                        │   │
│  │ │   └─ ファイル管理メタデータ                        │   │
│  │                                                     │   │
│  │ └─ 🔄 自動バックアップシステム                        │   │
│  │     ├─ GitHub Actions定期実行                        │   │
│  │     └─ 差分バックアップ機能                          │   │
│  └─────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────┘
                              │
                              ▼
                  ┌─────────────────────┐
                  │     📁 GitHub        │
                  │                     │
                  │ • data/フォルダ       │
                  │ • 自動コミット        │
                  │ • バージョン管理       │
                  │ • 無料100GB          │
                  └─────────────────────┘
```

#### 5.2.2 ChromaDBベクトルデータベース永続化

**1. ベクトルインデックス永続化設定**

```python
# utils/enhanced_rag_manager.py の永続化強化版

"""
RAGベクトルデータベース永続化システム
ChromaDB + ファイルシステム連携による完全データ保護
"""

import chromadb
from chromadb.config import Settings
from pathlib import Path
import json
import hashlib
from datetime import datetime

class PersistentRAGManager:
    """永続化対応RAG管理システム"""

    def __init__(self):
        # 永続化ディレクトリ設定
        self.persist_dir = Path("data/chroma_db")
        self.backup_dir = Path("data/chroma_backup")
        self.metadata_dir = Path("data/rag_metadata")

        # ディレクトリ作成
        for dir_path in [self.persist_dir, self.backup_dir, self.metadata_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)

        # ChromaDB永続化設定
        self.chroma_client = chromadb.PersistentClient(
            path=str(self.persist_dir),
            settings=Settings(
                anonymized_telemetry=False,
                allow_reset=False  # 永続化データの誤削除防止
            )
        )

        # コレクション管理
        self.collections = {}
        self._load_existing_collections()

    def _load_existing_collections(self):
        """既存のコレクションを復元"""
        try:
            # 永続化されたコレクション一覧を取得
            existing_collections = self.chroma_client.list_collections()

            for collection_info in existing_collections:
                collection_name = collection_info.name
                collection = self.chroma_client.get_collection(name=collection_name)
                self.collections[collection_name] = collection
                print(f"✅ コレクション復元: {collection_name}")

        except Exception as e:
            print(f"⚠️ コレクション復元エラー: {e}")

    def add_document_with_persistence(self, file_path: str, product_name: str):
        """ドキュメント追加 + 完全永続化"""
        try:
            # 1. ファイルハッシュ生成（重複チェック用）
            file_hash = self._calculate_file_hash(file_path)
            doc_id = f"{product_name}_{file_hash}"

            # 2. 既存チェック
            if self._document_exists(product_name, doc_id):
                print(f"📄 既存文書のため処理スキップ: {file_path}")
                return True

            # 3. 文書処理＆ベクトル化
            chunks = self._process_document(file_path)
            embeddings = self._create_embeddings(chunks)

            # 4. コレクション取得/作成
            collection = self._get_or_create_collection(product_name)

            # 5. ベクトルデータベースに保存
            collection.add(
                documents=chunks,
                embeddings=embeddings,
                ids=[f"{doc_id}_chunk_{i}" for i in range(len(chunks))],
                metadatas=[{
                    "source": file_path,
                    "product": product_name,
                    "chunk_id": i,
                    "file_hash": file_hash,
                    "created_at": datetime.now().isoformat()
                } for i in range(len(chunks))]
            )

            # 6. メタデータ永続化
            self._save_document_metadata(product_name, doc_id, file_path, chunks)

            # 7. 自動バックアップ
            self._create_incremental_backup(product_name)

            print(f"✅ 文書永続化完了: {file_path}")
            return True

        except Exception as e:
            print(f"❌ 文書永続化エラー: {e}")
            return False

    def _calculate_file_hash(self, file_path: str) -> str:
        """ファイルハッシュ計算（重複検出用）"""
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()[:8]

    def _document_exists(self, product_name: str, doc_id: str) -> bool:
        """文書重複チェック"""
        metadata_file = self.metadata_dir / f"{product_name}_metadata.json"
        if metadata_file.exists():
            with open(metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
                return doc_id in metadata.get("documents", {})
        return False

    def _save_document_metadata(self, product_name: str, doc_id: str, file_path: str, chunks: list):
        """文書メタデータ永続化"""
        metadata_file = self.metadata_dir / f"{product_name}_metadata.json"

        # 既存メタデータ読み込み
        if metadata_file.exists():
            with open(metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
        else:
            metadata = {"documents": {}, "created_at": datetime.now().isoformat()}

        # 新規文書情報追加
        metadata["documents"][doc_id] = {
            "file_path": file_path,
            "chunks_count": len(chunks),
            "processed_at": datetime.now().isoformat(),
            "file_size": Path(file_path).stat().st_size if Path(file_path).exists() else 0
        }
        metadata["last_updated"] = datetime.now().isoformat()

        # メタデータ保存
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)

    def _create_incremental_backup(self, product_name: str):
        """増分バックアップ作成"""
        try:
            backup_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_name = f"{product_name}_backup_{backup_timestamp}"

            # コレクションデータ取得
            if product_name in self.collections:
                collection = self.collections[product_name]
                all_data = collection.get()

                # バックアップファイル作成
                backup_file = self.backup_dir / f"{backup_name}.json"
                with open(backup_file, 'w', encoding='utf-8') as f:
                    json.dump({
                        "collection_name": product_name,
                        "backup_timestamp": backup_timestamp,
                        "documents_count": len(all_data.get("documents", [])),
                        "data": all_data
                    }, f, ensure_ascii=False, indent=2)

                print(f"🔄 増分バックアップ作成: {backup_name}")

        except Exception as e:
            print(f"⚠️ バックアップエラー: {e}")

    def get_storage_statistics(self):
        """ストレージ統計情報取得"""
        stats = {
            "total_collections": len(self.collections),
            "total_documents": 0,
            "storage_size_mb": 0,
            "collections_detail": {}
        }

        # ディスク使用量計算
        for path in [self.persist_dir, self.metadata_dir, self.backup_dir]:
            for file_path in path.rglob("*"):
                if file_path.is_file():
                    stats["storage_size_mb"] += file_path.stat().st_size

        stats["storage_size_mb"] = round(stats["storage_size_mb"] / (1024 * 1024), 2)

        # コレクション詳細
        for name, collection in self.collections.items():
            try:
                data = collection.get()
                doc_count = len(data.get("documents", []))
                stats["total_documents"] += doc_count
                stats["collections_detail"][name] = {
                    "documents": doc_count,
                    "last_updated": "N/A"  # メタデータから取得可能
                }
            except:
                stats["collections_detail"][name] = {"documents": 0, "last_updated": "Error"}

        return stats
```

**2. 自動復元システム**

```python
# config/auto_recovery.py

"""
自動データ復元システム
アプリ起動時の自動データ復旧機能
"""

class AutoRecoverySystem:
    """自動復元システム"""

    def __init__(self):
        self.data_dir = Path("data")
        self.recovery_log = Path("data/recovery.log")

    def startup_recovery_check(self):
        """起動時復元チェック"""
        print("🔍 データ整合性チェック開始...")

        issues_found = []

        # 1. ChromaDBデータ確認
        chroma_dir = self.data_dir / "chroma_db"
        if not chroma_dir.exists():
            issues_found.append("ChromaDBディレクトリ不存在")
            self._restore_chroma_from_backup()

        # 2. SQLiteデータベース確認
        db_file = self.data_dir / "chatbot.db"
        if not db_file.exists():
            issues_found.append("SQLiteデータベース不存在")
            self._initialize_database()

        # 3. メタデータ整合性チェック
        inconsistencies = self._check_metadata_consistency()
        if inconsistencies:
            issues_found.extend(inconsistencies)

        # 4. 復元実行
        if issues_found:
            print(f"⚠️ {len(issues_found)}件の問題を検出、自動復元実行中...")
            self._execute_recovery(issues_found)
        else:
            print("✅ データ整合性確認完了")

    def _restore_chroma_from_backup(self):
        """ChromaDBバックアップからの復元"""
        backup_dir = self.data_dir / "chroma_backup"
        if backup_dir.exists():
            latest_backup = max(backup_dir.glob("*.json"), key=lambda p: p.stat().st_mtime, default=None)
            if latest_backup:
                print(f"🔄 ChromaDB復元中: {latest_backup.name}")
                # 復元処理実装
```

### 5.2 永続化設定（15分で完了）

#### 5.2.1 自動永続化システムの実装

以下のファイルを作成して永続化を実現します：

**1. `.gitignore`ファイルの確認**

Wiki Chatbotフォルダに`.gitignore`ファイルを作成し、以下を記述：

```gitignore
# Python
__pycache__/
*.pyc
*.pyo
*.pyd
.Python

# 秘密情報
.env
secrets.toml

# 一時ファイル
*.tmp
*.log

# 永続化したいファイルはGitHubに含める
# data/ フォルダは永続化対象
!data/
!data/**/*
```

**2. 永続化管理システム**

`config/persistent_storage.py`を作成：

```python
"""
永続化ストレージ管理システム
GitHub + Streamlit Cloud連携による自動データ保存
"""

import os
import json
import shutil
from pathlib import Path
from datetime import datetime
import streamlit as st

class PersistentStorage:
    """永続化ストレージ管理クラス"""

    def __init__(self):
        self.data_dir = Path("data")
        self.backup_dir = Path("data/backups")
        self.max_storage_mb = 100  # 最大100MB

        # ディレクトリ作成
        self.data_dir.mkdir(exist_ok=True)
        self.backup_dir.mkdir(exist_ok=True)

    def save_uploaded_file(self, uploaded_file, product_name: str):
        """アップロードファイルを永続化"""
        try:
            # 製品別フォルダ作成
            product_dir = self.data_dir / "products" / product_name
            product_dir.mkdir(parents=True, exist_ok=True)

            # ファイル保存
            file_path = product_dir / uploaded_file.name
            with open(file_path, "wb") as f:
                f.write(uploaded_file.getbuffer())

            # メタデータ保存
            self._save_metadata(file_path, uploaded_file)

            return str(file_path)

        except Exception as e:
            st.error(f"ファイル保存エラー: {e}")
            return None

    def _save_metadata(self, file_path: Path, uploaded_file):
        """ファイルメタデータを保存"""
        metadata = {
            "original_name": uploaded_file.name,
            "size": uploaded_file.size,
            "type": uploaded_file.type,
            "uploaded_at": datetime.now().isoformat(),
            "file_path": str(file_path)
        }

        metadata_path = file_path.with_suffix(f"{file_path.suffix}.meta")
        with open(metadata_path, "w", encoding="utf-8") as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)

    def get_storage_usage(self):
        """ストレージ使用量を取得"""
        total_size = 0
        for file_path in self.data_dir.rglob("*"):
            if file_path.is_file():
                total_size += file_path.stat().st_size

        return total_size / (1024 * 1024)  # MB変換

    def cleanup_old_files(self, days_old: int = 30):
        """古いファイルを自動削除"""
        cutoff_time = datetime.now().timestamp() - (days_old * 24 * 3600)

        for file_path in self.data_dir.rglob("*"):
            if file_path.is_file() and file_path.stat().st_mtime < cutoff_time:
                # バックアップに移動
                backup_path = self.backup_dir / file_path.name
                shutil.move(str(file_path), str(backup_path))

    def create_backup(self):
        """データベース全体のバックアップ作成"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_name = f"backup_{timestamp}"

        shutil.make_archive(
            str(self.backup_dir / backup_name),
            "zip",
            str(self.data_dir)
        )

# グローバルインスタンス
persistent_storage = PersistentStorage()
```

**3. ファイルハンドラーの更新**

`utils/file_handler.py`に永続化機能を追加：

```python
# 既存のsave_uploaded_file関数を更新
def save_uploaded_file(self, uploaded_file, product_name: str):
    """永続化対応アップロードファイル保存"""
    from config.persistent_storage import persistent_storage

    # 永続化ストレージに保存
    saved_path = persistent_storage.save_uploaded_file(uploaded_file, product_name)

    if saved_path:
        # RAGインデックスに追加
        success = self.rag_manager.add_document(
            file_path=saved_path,
            product_name=product_name
        )

        if success:
            st.success(f"✅ {uploaded_file.name} を永続化しました")
            return True

    return False
```

#### 5.2.2 自動バックアップ設定

**GitHub Actions設定** (`.github/workflows/backup.yml`)

```yaml
name: Auto Backup
on:
  schedule:
    - cron: '0 2 * * *'  # 毎日午前2時
  workflow_dispatch:

jobs:
  backup:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v2
    - name: Create Backup
      run: |
        echo "$(date): Backup completed" >> data/backup_log.txt
    - name: Commit Backup
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add data/
        git commit -m "Auto backup: $(date)" || exit 0
        git push
```

### 5.3 容量制限管理

```python
# config/storage_limits.py
STORAGE_LIMITS = {
    "max_total_mb": 100,        # 合計100MB
    "max_file_mb": 10,          # ファイル1つ10MB
    "max_files_per_product": 50, # 商品毎50ファイル
    "auto_cleanup_days": 90,    # 90日で自動削除
    "backup_retention_days": 30  # バックアップ30日保存
}
```

---

## 6. 💬 会話履歴を永久保存

### 6.1 📊 ユーザーインタラクション永続化の全体像

#### 🌟 「なぜ会話履歴が重要なの？」完全解説

**1. 会話の継続性確保**
```
🗣️ 自然な会話体験の実現

❌ 履歴なしの場合:
  ユーザー: 「先ほどの件について...」
  AI: 「すみません、覚えていません」😰

✅ 履歴ありの場合:
  ユーザー: 「先ほどの件について...」
  AI: 「はい、○○の件ですね！」😊
```

**2. 学習効果とパフォーマンス向上**
```
📈 システム改善への活用

✅ 質問パターン分析
   ├─ よく聞かれる質問の特定
   ├─ 回答精度の測定
   └─ システム改善点の発見

✅ ユーザー満足度分析
   ├─ 満足度評価の蓄積
   ├─ 問題のある回答の特定
   └─ AI回答品質の向上

✅ 運用最適化
   ├─ ピーク時間の把握
   ├─ リソース配分の最適化
   └─ コスト効率の改善
```

**3. 企業利用での価値**
```
🏢 ビジネス活用例

📋 知識蓄積
   ├─ 社内Q&Aの構築
   ├─ よくある質問の整理
   └─ 新人教育資料作成

📊 分析レポート
   ├─ 部署別利用状況
   ├─ 問い合わせ傾向分析
   └─ 業務効率化の指標

🔍 監査対応
   ├─ 情報アクセス履歴
   ├─ コンプライアンス確認
   └─ セキュリティ分析
```

### 6.2 🗄️ 完全永続化データベースシステム

#### 6.2.1 多層データベース構造

```
🏗️ データ永続化アーキテクチャ

┌─────────────────────────────────────────────────────────┐
│                  💾 永続化データ管理層                   │
│                                                         │
│  ┌─────────────────┐  ┌─────────────────────────────────┐ │
│  │  🗣️ リアルタイム  │  │        📊 分析・統計データ        │ │
│  │   インタラクション │  │                                 │ │
│  │                 │  │ • 利用統計テーブル               │ │
│  │ • セッション管理   │  │ • 満足度分析テーブル             │ │
│  │ • 会話履歴       │  │ • パフォーマンステーブル           │ │
│  │ • ユーザー状態    │  │ • 月次/週次集計テーブル           │ │
│  └─────────────────┘  └─────────────────────────────────┘ │
│           │                              │               │
│           ▼                              ▼               │
│  ┌─────────────────────────────────────────────────────┐   │
│  │           🗃️ SQLite永続化データベース                │   │
│  │                                                     │   │
│  │ 📋 chat_history              💭 user_feedback        │   │
│  │  ├─ conversation_id           ├─ feedback_id         │   │
│  │  ├─ user_message             ├─ satisfaction_score  │   │
│  │  ├─ ai_response              ├─ improvement_note    │   │
│  │  ├─ context_used             ├─ feature_request     │   │
│  │  ├─ response_time            └─ timestamp           │   │
│  │  ├─ satisfaction_score                              │   │
│  │  └─ metadata (JSON)          🔗 session_management  │   │
│  │                               ├─ session_id         │   │
│  │ 📁 file_interactions          ├─ user_identifier    │   │
│  │  ├─ file_access_log           ├─ login_time         │   │
│  │  ├─ upload_history            ├─ last_activity      │   │
│  │  ├─ download_requests         └─ session_duration   │   │
│  │  └─ permission_log                                  │   │
│  └─────────────────────────────────────────────────────┘   │
│                              │                             │
│                              ▼                             │
│  ┌─────────────────────────────────────────────────────┐   │
│  │              🔄 自動バックアップ・復元                │   │
│  │                                                     │   │
│  │ • 毎日自動バックアップ        • 障害時自動復元         │   │
│  │ • 差分バックアップ           • データ整合性チェック     │   │
│  │ • 世代管理（30日間）         • 緊急時手動復元         │   │
│  └─────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────┘
```

#### 6.2.2 高度なデータベース設計

**完全版データベーススキーマ** (`config/database.py`)

```python
"""
包括的データ永続化システム
ユーザーインタラクション完全記録・分析システム
"""

import sqlite3
import json
import hashlib
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
import logging

class ComprehensiveDatabase:
    """包括的データベース管理システム"""

    def __init__(self):
        self.db_path = Path("data/chatbot_comprehensive.db")
        self.backup_dir = Path("data/db_backups")
        self.db_path.parent.mkdir(exist_ok=True)
        self.backup_dir.mkdir(exist_ok=True)

        # ログ設定
        self.logger = self._setup_logging()

        # データベース初期化
        self._init_comprehensive_database()

        # 定期バックアップ設定
        self._setup_auto_backup()

    def _init_comprehensive_database(self):
        """包括的データベース構造の初期化"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("PRAGMA foreign_keys = ON")  # 外部キー制約有効化

            # 1. 詳細会話履歴テーブル
            conn.execute("""
                CREATE TABLE IF NOT EXISTS conversation_history (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    conversation_id TEXT NOT NULL,
                    session_id TEXT NOT NULL,
                    timestamp TEXT NOT NULL,

                    -- ユーザー情報
                    user_ip TEXT,
                    user_agent TEXT,
                    user_session_duration INTEGER,

                    -- 対話内容
                    product_name TEXT NOT NULL,
                    user_message TEXT NOT NULL,
                    user_message_length INTEGER,
                    ai_response TEXT NOT NULL,
                    ai_response_length INTEGER,

                    -- RAG情報
                    sources_used TEXT,  -- JSON形式
                    chunks_retrieved INTEGER,
                    similarity_scores TEXT,  -- JSON形式

                    -- 処理詳細
                    llm_model_used TEXT,
                    prompt_style TEXT,
                    processing_time_ms INTEGER,
                    tokens_used INTEGER,
                    cost_estimate REAL,

                    -- 満足度・評価
                    satisfaction_score INTEGER,
                    user_feedback_immediate TEXT,
                    response_quality_score REAL,

                    -- メタデータ
                    error_occurred BOOLEAN DEFAULT FALSE,
                    error_message TEXT,
                    retry_count INTEGER DEFAULT 0,
                    metadata TEXT,  -- JSON形式

                    created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                    updated_at TEXT DEFAULT CURRENT_TIMESTAMP
                )
            """)

            # 2. ユーザーセッション管理テーブル
            conn.execute("""
                CREATE TABLE IF NOT EXISTS user_sessions (
                    session_id TEXT PRIMARY KEY,
                    start_time TEXT NOT NULL,
                    end_time TEXT,
                    user_ip TEXT,
                    user_agent TEXT,

                    -- セッション統計
                    total_messages INTEGER DEFAULT 0,
                    total_files_uploaded INTEGER DEFAULT 0,
                    total_time_spent_minutes INTEGER DEFAULT 0,
                    products_accessed TEXT,  -- JSON配列

                    -- 行動パターン
                    peak_activity_hour INTEGER,
                    average_response_time_ms REAL,
                    satisfaction_average REAL,

                    -- セッション品質
                    successful_interactions INTEGER DEFAULT 0,
                    failed_interactions INTEGER DEFAULT 0,
                    session_quality_score REAL,

                    created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                    last_updated TEXT DEFAULT CURRENT_TIMESTAMP
                )
            """)

            # 3. ファイル操作履歴テーブル
            conn.execute("""
                CREATE TABLE IF NOT EXISTS file_interaction_log (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    session_id TEXT NOT NULL,
                    action_type TEXT NOT NULL,  -- upload, access, delete, etc.

                    -- ファイル情報
                    product_name TEXT NOT NULL,
                    file_name TEXT NOT NULL,
                    file_path TEXT,
                    file_size_bytes INTEGER,
                    file_type TEXT,
                    file_hash TEXT,  -- 重複検出用

                    -- 処理詳細
                    processing_time_ms INTEGER,
                    success BOOLEAN NOT NULL,
                    error_message TEXT,
                    chunks_created INTEGER,

                    -- アクセス制御
                    user_ip TEXT,
                    permission_level TEXT,
                    access_reason TEXT,

                    timestamp TEXT DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (session_id) REFERENCES user_sessions (session_id)
                )
            """)

            # 4. 詳細フィードバックテーブル
            conn.execute("""
                CREATE TABLE IF NOT EXISTS user_feedback_detailed (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    conversation_id TEXT NOT NULL,
                    session_id TEXT NOT NULL,

                    -- 満足度詳細
                    overall_satisfaction INTEGER,  -- 1-5スケール
                    response_accuracy INTEGER,
                    response_speed INTEGER,
                    response_helpfulness INTEGER,
                    ui_experience INTEGER,

                    -- フリーテキストフィードバック
                    positive_feedback TEXT,
                    improvement_suggestions TEXT,
                    feature_requests TEXT,
                    bug_reports TEXT,

                    -- 分類・タグ
                    feedback_category TEXT,
                    priority_level TEXT,
                    status TEXT DEFAULT 'new',  -- new, reviewed, addressed

                    created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (conversation_id) REFERENCES conversation_history (conversation_id)
                )
            """)

            # 5. システム利用統計テーブル
            conn.execute("""
                CREATE TABLE IF NOT EXISTS usage_analytics (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    date TEXT NOT NULL,  -- YYYY-MM-DD形式

                    -- 利用量統計
                    total_sessions INTEGER DEFAULT 0,
                    total_conversations INTEGER DEFAULT 0,
                    total_files_uploaded INTEGER DEFAULT 0,
                    total_users INTEGER DEFAULT 0,

                    -- パフォーマンス統計
                    avg_response_time_ms REAL,
                    avg_satisfaction_score REAL,
                    success_rate REAL,
                    error_rate REAL,

                    -- リソース使用量
                    total_tokens_used INTEGER DEFAULT 0,
                    total_cost_estimate REAL,
                    storage_used_mb REAL,

                    -- ピーク時間分析
                    peak_hour INTEGER,
                    peak_hour_sessions INTEGER,

                    created_at TEXT DEFAULT CURRENT_TIMESTAMP
                )
            """)

            # 6. エラー・ログテーブル
            conn.execute("""
                CREATE TABLE IF NOT EXISTS system_logs (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT NOT NULL,
                    log_level TEXT NOT NULL,  -- DEBUG, INFO, WARNING, ERROR, CRITICAL

                    -- エラー詳細
                    component TEXT,  -- RAG, LLM, Database, etc.
                    error_type TEXT,
                    error_message TEXT,
                    stack_trace TEXT,

                    -- コンテキスト
                    session_id TEXT,
                    user_ip TEXT,
                    affected_operation TEXT,

                    -- 解決状況
                    resolution_status TEXT DEFAULT 'new',
                    resolution_notes TEXT,
                    resolved_at TEXT,

                    created_at TEXT DEFAULT CURRENT_TIMESTAMP
                )
            """)

    def save_comprehensive_interaction(self, interaction_data: Dict) -> bool:
        """包括的インタラクションデータ保存"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                # 1. 会話履歴保存
                conn.execute("""
                    INSERT INTO conversation_history (
                        conversation_id, session_id, timestamp, user_ip, user_agent,
                        product_name, user_message, user_message_length,
                        ai_response, ai_response_length,
                        sources_used, chunks_retrieved, similarity_scores,
                        llm_model_used, prompt_style, processing_time_ms,
                        tokens_used, cost_estimate, satisfaction_score,
                        metadata
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    interaction_data.get('conversation_id'),
                    interaction_data.get('session_id'),
                    datetime.now().isoformat(),
                    interaction_data.get('user_ip'),
                    interaction_data.get('user_agent'),
                    interaction_data.get('product_name'),
                    interaction_data.get('user_message'),
                    len(interaction_data.get('user_message', '')),
                    interaction_data.get('ai_response'),
                    len(interaction_data.get('ai_response', '')),
                    json.dumps(interaction_data.get('sources_used', [])),
                    interaction_data.get('chunks_retrieved', 0),
                    json.dumps(interaction_data.get('similarity_scores', [])),
                    interaction_data.get('llm_model_used'),
                    interaction_data.get('prompt_style'),
                    interaction_data.get('processing_time_ms'),
                    interaction_data.get('tokens_used'),
                    interaction_data.get('cost_estimate'),
                    interaction_data.get('satisfaction_score'),
                    json.dumps(interaction_data.get('metadata', {}))
                ))

                # 2. セッション統計更新
                self._update_session_stats(
                    conn,
                    interaction_data.get('session_id'),
                    interaction_data
                )

                conn.commit()
                return True

        except Exception as e:
            self.logger.error(f"包括的インタラクションデータ保存エラー: {e}")
            return False

    def _update_session_stats(self, conn, session_id: str, interaction_data: Dict):
        """セッション統計の更新"""
        # セッション存在確認
        cursor = conn.execute(
            "SELECT total_messages FROM user_sessions WHERE session_id = ?",
            (session_id,)
        )
        result = cursor.fetchone()

        if result:
            # 既存セッション更新
            conn.execute("""
                UPDATE user_sessions SET
                    total_messages = total_messages + 1,
                    last_updated = ?,
                    satisfaction_average = (
                        SELECT AVG(satisfaction_score)
                        FROM conversation_history
                        WHERE session_id = ? AND satisfaction_score IS NOT NULL
                    )
                WHERE session_id = ?
            """, (datetime.now().isoformat(), session_id, session_id))
        else:
            # 新規セッション作成
            conn.execute("""
                INSERT INTO user_sessions (
                    session_id, start_time, user_ip, user_agent, total_messages
                ) VALUES (?, ?, ?, ?, 1)
            """, (
                session_id,
                datetime.now().isoformat(),
                interaction_data.get('user_ip'),
                interaction_data.get('user_agent')
            ))

    def get_comprehensive_analytics(self, date_range: int = 30) -> Dict:
        """包括的分析データ取得"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                # 指定期間のデータ取得
                cutoff_date = (datetime.now() - timedelta(days=date_range)).isoformat()

                analytics = {}

                # 1. 基本統計
                cursor = conn.execute("""
                    SELECT
                        COUNT(*) as total_conversations,
                        COUNT(DISTINCT session_id) as unique_sessions,
                        AVG(satisfaction_score) as avg_satisfaction,
                        AVG(processing_time_ms) as avg_processing_time,
                        SUM(tokens_used) as total_tokens
                    FROM conversation_history
                    WHERE created_at >= ?
                """, (cutoff_date,))

                basic_stats = cursor.fetchone()
                analytics['basic_stats'] = {
                    'total_conversations': basic_stats[0],
                    'unique_sessions': basic_stats[1],
                    'avg_satisfaction': round(basic_stats[2] or 0, 2),
                    'avg_processing_time_ms': round(basic_stats[3] or 0, 2),
                    'total_tokens': basic_stats[4] or 0
                }

                # 2. 商品別利用状況
                cursor = conn.execute("""
                    SELECT
                        product_name,
                        COUNT(*) as conversation_count,
                        AVG(satisfaction_score) as avg_satisfaction
                    FROM conversation_history
                    WHERE created_at >= ?
                    GROUP BY product_name
                    ORDER BY conversation_count DESC
                """, (cutoff_date,))

                analytics['product_usage'] = [
                    {
                        'product': row[0],
                        'conversations': row[1],
                        'satisfaction': round(row[2] or 0, 2)
                    }
                    for row in cursor.fetchall()
                ]

                # 3. 時間帯別利用パターン
                cursor = conn.execute("""
                    SELECT
                        strftime('%H', created_at) as hour,
                        COUNT(*) as conversation_count
                    FROM conversation_history
                    WHERE created_at >= ?
                    GROUP BY strftime('%H', created_at)
                    ORDER BY hour
                """, (cutoff_date,))

                analytics['hourly_usage'] = {
                    row[0]: row[1] for row in cursor.fetchall()
                }

                return analytics

        except Exception as e:
            self.logger.error(f"分析データ取得エラー: {e}")
            return {}

    def export_data_for_analysis(self, format_type: str = 'csv') -> str:
        """分析用データエクスポート"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            export_dir = Path("data/exports")
            export_dir.mkdir(exist_ok=True)

            if format_type == 'csv':
                import csv
                export_file = export_dir / f"chat_analytics_{timestamp}.csv"

                with sqlite3.connect(self.db_path) as conn:
                    cursor = conn.execute("""
                        SELECT
                            conversation_id, session_id, timestamp, product_name,
                            user_message_length, ai_response_length,
                            satisfaction_score, processing_time_ms, tokens_used
                        FROM conversation_history
                        ORDER BY timestamp DESC
                    """)

                    with open(export_file, 'w', newline='', encoding='utf-8') as csvfile:
                        writer = csv.writer(csvfile)
                        writer.writerow([
                            'Conversation ID', 'Session ID', 'Timestamp', 'Product',
                            'User Message Length', 'AI Response Length',
                            'Satisfaction', 'Processing Time (ms)', 'Tokens Used'
                        ])
                        writer.writerows(cursor.fetchall())

                return str(export_file)

        except Exception as e:
            self.logger.error(f"データエクスポートエラー: {e}")
            return ""
```

#### 6.2.3 リアルタイム分析ダッシュボード

**管理画面向け分析機能** (`pages/analytics_dashboard.py`)

```python
"""
リアルタイム分析ダッシュボード
ユーザーインタラクション可視化システム
"""

import streamlit as st
import plotly.express as px
import plotly.graph_objects as go
import pandas as pd
from datetime import datetime, timedelta

def render_analytics_dashboard():
    """分析ダッシュボードのレンダリング"""

    st.title("📊 ユーザーインタラクション分析ダッシュボード")

    # データ取得
    analytics_data = get_comprehensive_analytics()

    # 1. KPI表示
    col1, col2, col3, col4 = st.columns(4)

    with col1:
        st.metric(
            "総会話数",
            analytics_data['basic_stats']['total_conversations'],
            delta=f"+{get_daily_growth('conversations')}%"
        )

    with col2:
        st.metric(
            "平均満足度",
            f"{analytics_data['basic_stats']['avg_satisfaction']}/5.0",
            delta=f"+{get_satisfaction_trend()}%"
        )

    with col3:
        st.metric(
            "アクティブセッション",
            analytics_data['basic_stats']['unique_sessions'],
            delta=f"+{get_daily_growth('sessions')}%"
        )

    with col4:
        st.metric(
            "平均応答時間",
            f"{analytics_data['basic_stats']['avg_processing_time_ms']:.0f}ms",
            delta=f"-{get_performance_improvement()}%"
        )

    # 2. 時間帯別利用パターン
    st.subheader("⏰ 時間帯別利用パターン")

    hourly_data = analytics_data.get('hourly_usage', {})
    hours = list(range(24))
    usage_counts = [hourly_data.get(f"{h:02d}", 0) for h in hours]

    fig_hourly = px.bar(
        x=hours,
        y=usage_counts,
        title="時間帯別会話数",
        labels={'x': '時間', 'y': '会話数'}
    )
    st.plotly_chart(fig_hourly, use_container_width=True)

    # 3. 商品別利用状況
    st.subheader("📦 商品別利用状況")

    product_data = analytics_data.get('product_usage', [])
    if product_data:
        df_products = pd.DataFrame(product_data)

        fig_products = px.pie(
            df_products,
            values='conversations',
            names='product',
            title="商品別会話分布"
        )
        st.plotly_chart(fig_products, use_container_width=True)

    # 4. 満足度トレンド
    st.subheader("😊 満足度トレンド")

    satisfaction_trend = get_satisfaction_trend_data()
    if satisfaction_trend:
        fig_satisfaction = px.line(
            satisfaction_trend,
            x='date',
            y='avg_satisfaction',
            title="日別平均満足度",
            labels={'date': '日付', 'avg_satisfaction': '平均満足度'}
        )
        st.plotly_chart(fig_satisfaction, use_container_width=True)
```

### 6.3 🔧 実装手順（コピペで完了）

#### 6.3.1 データベース永続化の実装

**1. 設定ファイル更新**

`config/settings.py`に以下を追加：

```python
# データベース永続化設定
DATABASE_CONFIG = {
    "enable_persistence": True,
    "db_path": "data/chatbot_comprehensive.db",
    "backup_interval_hours": 24,
    "max_backup_files": 30,
    "auto_vacuum": True,
    "analytics_enabled": True
}

# データ保持ポリシー
DATA_RETENTION_POLICY = {
    "chat_history_days": 365,      # 1年間保持
    "user_sessions_days": 180,     # 6ヶ月保持
    "system_logs_days": 90,        # 3ヶ月保持
    "analytics_data_days": 730     # 2年間保持
}
```

**2. メインアプリケーション統合**

`app.py`に永続化機能を統合：

```python
import streamlit as st
from config.database import ComprehensiveDatabase
from utils.session_manager import SessionManager
import uuid
from datetime import datetime

# アプリ起動時の初期化
@st.cache_resource
def initialize_persistent_database():
    """永続化データベースの初期化"""
    return ComprehensiveDatabase()

def save_interaction_with_persistence(user_message, ai_response, product_name):
    """インタラクションデータの永続化保存"""

    db = initialize_persistent_database()
    session_manager = SessionManager()

    # セッション情報取得
    session_id = session_manager.get_session_id()

    # インタラクションデータ構築
    interaction_data = {
        'conversation_id': str(uuid.uuid4()),
        'session_id': session_id,
        'user_ip': st.context.headers.get('x-forwarded-for', 'unknown'),
        'user_agent': st.context.headers.get('user-agent', 'unknown'),
        'product_name': product_name,
        'user_message': user_message,
        'ai_response': ai_response,
        'llm_model_used': st.session_state.get('current_llm_model', 'unknown'),
        'prompt_style': st.session_state.get('prompt_style', 'standard'),
        'processing_time_ms': st.session_state.get('last_response_time', 0),
        'tokens_used': st.session_state.get('tokens_used', 0),
        'sources_used': st.session_state.get('sources_used', []),
        'chunks_retrieved': len(st.session_state.get('sources_used', [])),
        'metadata': {
            'timestamp': datetime.now().isoformat(),
            'page': 'chat',
            'feature_flags': st.session_state.get('feature_flags', {})
        }
    }

    # データベース保存
    success = db.save_comprehensive_interaction(interaction_data)

    if success:
        st.session_state['last_interaction_saved'] = True

    return success
```

**3. ユーザーフィードバック機能の統合**

```python
def save_user_feedback(conversation_id, satisfaction_data):
    """ユーザーフィードバックの保存"""

    db = initialize_persistent_database()

    with sqlite3.connect(db.db_path) as conn:
        conn.execute("""
            INSERT INTO user_feedback_detailed (
                conversation_id, session_id, overall_satisfaction,
                response_accuracy, response_speed, response_helpfulness,
                positive_feedback, improvement_suggestions
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            conversation_id,
            st.session_state.get('session_id'),
            satisfaction_data.get('overall', 3),
            satisfaction_data.get('accuracy', 3),
            satisfaction_data.get('speed', 3),
            satisfaction_data.get('helpfulness', 3),
            satisfaction_data.get('positive_feedback', ''),
            satisfaction_data.get('improvements', '')
        ))

# チャット画面にフィードバック機能追加
def render_feedback_widget():
    """フィードバックウィジェットのレンダリング"""

    if 'last_ai_response' in st.session_state:
        st.markdown("---")
        st.subheader("📝 この回答はいかがでしたか？")

        col1, col2, col3, col4, col5 = st.columns(5)

        satisfaction_scores = {}

        with col1:
            satisfaction_scores['overall'] = st.select_slider(
                "総合満足度", options=[1,2,3,4,5], value=3, key="overall_satisfaction"
            )

        with col2:
            satisfaction_scores['accuracy'] = st.select_slider(
                "回答精度", options=[1,2,3,4,5], value=3, key="accuracy_satisfaction"
            )

        with col3:
            satisfaction_scores['speed'] = st.select_slider(
                "応答速度", options=[1,2,3,4,5], value=3, key="speed_satisfaction"
            )

        with col4:
            satisfaction_scores['helpfulness'] = st.select_slider(
                "有用性", options=[1,2,3,4,5], value=3, key="helpfulness_satisfaction"
            )

        # フリーテキストフィードバック
        positive_feedback = st.text_area(
            "良かった点をお聞かせください",
            placeholder="この回答で役に立った部分や良かった点があれば教えてください"
        )

        improvement_feedback = st.text_area(
            "改善提案があればお聞かせください",
            placeholder="より良い回答にするためのご提案があれば教えてください"
        )

        if st.button("📤 フィードバック送信"):
            feedback_data = {
                **satisfaction_scores,
                'positive_feedback': positive_feedback,
                'improvements': improvement_feedback
            }

            # フィードバック保存
            if save_user_feedback(st.session_state.get('last_conversation_id'), feedback_data):
                st.success("フィードバックを送信しました。ありがとうございます！")
                st.balloons()
            else:
                st.error("フィードバックの送信に失敗しました")
```

#### 6.3.2 自動バックアップシステム

**GitHub Actions自動バックアップ設定**

`.github/workflows/data_backup.yml`を作成：

```yaml
name: 🔄 Automated Data Backup & Sync

on:
  schedule:
    # 毎日午前3時(JST)に実行
    - cron: '0 18 * * *'  # UTC 18:00 = JST 03:00
  workflow_dispatch:
    inputs:
      backup_type:
        description: 'バックアップの種類'
        required: true
        default: 'incremental'
        type: choice
        options:
        - incremental
        - full
        - emergency

jobs:
  automated_backup:
    runs-on: ubuntu-latest

    steps:
    - name: 📥 リポジトリをチェックアウト
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        fetch-depth: 0

    - name: 📊 データファイル存在確認
      run: |
        echo "=== データディレクトリ構造確認 ==="
        if [ -d "data" ]; then
          find data -type f -name "*.db" -o -name "*.json" -o -name "*.csv" | head -20
          du -sh data/ || echo "データディレクトリサイズ測定不可"
        else
          echo "dataディレクトリが存在しません"
          mkdir -p data
        fi

    - name: 🔄 データベース整合性チェック
      run: |
        echo "=== データベース整合性チェック ==="

        # SQLiteデータベースの整合性チェック
        for db_file in data/*.db; do
          if [ -f "$db_file" ]; then
            echo "チェック中: $db_file"
            # SQLiteファイルの破損チェック（基本的な確認）
            if file "$db_file" | grep -q "SQLite"; then
              echo "✅ $db_file は有効なSQLiteデータベースです"
            else
              echo "⚠️ $db_file は破損している可能性があります"
            fi
          fi
        done

    - name: 📋 バックアップメタデータ生成
      run: |
        echo "=== バックアップメタデータ生成 ==="

        backup_timestamp=$(date '+%Y-%m-%d %H:%M:%S UTC')
        backup_type="${{ github.event.inputs.backup_type || 'incremental' }}"

        # メタデータファイル作成
        cat > data/backup_metadata.json << EOF
        {
          "backup_timestamp": "$backup_timestamp",
          "backup_type": "$backup_type",
          "triggered_by": "${{ github.event_name }}",
          "repository": "${{ github.repository }}",
          "commit_sha": "${{ github.sha }}",
          "workflow_run_id": "${{ github.run_id }}",
          "data_summary": {
            "total_files": $(find data -type f | wc -l),
            "database_files": $(find data -name "*.db" | wc -l),
            "json_files": $(find data -name "*.json" | wc -l),
            "backup_files": $(find data -name "*backup*" | wc -l)
          }
        }
        EOF

        echo "📄 バックアップメタデータ:"
        cat data/backup_metadata.json

    - name: 🧹 古いバックアップファイルクリーンアップ
      run: |
        echo "=== 古いバックアップファイルの削除 ==="

        # 30日以上古いバックアップファイルを削除
        find data -name "*backup*" -type f -mtime +30 -delete 2>/dev/null || true

        # ログファイルのローテーション（100ファイル以上で古いものを削除）
        log_count=$(find data -name "*.log" | wc -l)
        if [ "$log_count" -gt 100 ]; then
          find data -name "*.log" -type f -printf '%T@ %p\n' | sort -n | head -50 | cut -d' ' -f2- | xargs rm -f
          echo "古いログファイルを削除しました"
        fi

    - name: 📝 変更サマリー生成
      run: |
        echo "=== 変更サマリー生成 ==="

        # Git status確認
        git status --porcelain

        # 変更があるかチェック
        if [ -n "$(git status --porcelain)" ]; then
          echo "📊 変更が検出されました"
          git add data/

          # コミットメッセージ生成
          commit_message="🔄 自動データバックアップ: $(date '+%Y-%m-%d %H:%M:%S')"

          if [ "${{ github.event.inputs.backup_type }}" = "emergency" ]; then
            commit_message="🚨 緊急バックアップ: $(date '+%Y-%m-%d %H:%M:%S')"
          fi

          echo "COMMIT_MESSAGE=$commit_message" >> $GITHUB_ENV
          echo "HAS_CHANGES=true" >> $GITHUB_ENV
        else
          echo "📭 変更なし - コミット不要"
          echo "HAS_CHANGES=false" >> $GITHUB_ENV
        fi

    - name: 🚀 変更をコミット・プッシュ
      if: env.HAS_CHANGES == 'true'
      run: |
        echo "=== Git設定 ==="
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Actions Data Backup"

        echo "=== コミット実行 ==="
        git commit -m "${{ env.COMMIT_MESSAGE }}"

        echo "=== プッシュ実行 ==="
        git push

    - name: 📬 バックアップ完了通知
      if: always()
      run: |
        echo "=== バックアップ処理完了 ==="
        echo "実行時刻: $(date)"
        echo "バックアップタイプ: ${{ github.event.inputs.backup_type || 'incremental' }}"
        echo "変更あり: ${{ env.HAS_CHANGES || 'false' }}"
        echo "ワークフロー実行ID: ${{ github.run_id }}"

        # 実行ログファイル作成
        echo "$(date '+%Y-%m-%d %H:%M:%S') - バックアップ完了" >> data/backup_execution.log
```

#### 6.3.3 データエクスポート・分析機能

**管理画面でのデータ分析機能追加**

`pages/admin.py`に以下の機能を追加：

```python
def render_data_analytics_section():
    """データ分析セクションのレンダリング"""

    st.header("📊 データ分析・エクスポート")

    # データベース統計表示
    db = initialize_persistent_database()
    analytics = db.get_comprehensive_analytics()

    # 1. 基本統計表示
    st.subheader("📈 基本統計（過去30日）")

    col1, col2, col3, col4 = st.columns(4)

    with col1:
        st.metric(
            "総会話数",
            analytics['basic_stats']['total_conversations'],
            help="過去30日間の総会話数"
        )

    with col2:
        st.metric(
            "ユニークセッション",
            analytics['basic_stats']['unique_sessions'],
            help="過去30日間のユニークセッション数"
        )

    with col3:
        st.metric(
            "平均満足度",
            f"{analytics['basic_stats']['avg_satisfaction']:.1f}/5.0",
            help="ユーザー満足度の平均値"
        )

    with col4:
        st.metric(
            "平均応答時間",
            f"{analytics['basic_stats']['avg_processing_time_ms']:.0f}ms",
            help="AI応答の平均処理時間"
        )

    # 2. データエクスポート機能
    st.subheader("📤 データエクスポート")

    col1, col2 = st.columns(2)

    with col1:
        export_format = st.selectbox(
            "エクスポート形式",
            ["CSV", "JSON", "Excel"],
            help="エクスポートするデータの形式を選択"
        )

    with col2:
        date_range = st.selectbox(
            "期間",
            ["過去7日", "過去30日", "過去90日", "全期間"],
            help="エクスポートするデータの期間を選択"
        )

    if st.button("📊 データエクスポート実行"):
        with st.spinner("データエクスポート中..."):
            try:
                # エクスポート実行
                export_file = db.export_data_for_analysis(export_format.lower())

                if export_file:
                    st.success(f"✅ エクスポート完了: {export_file}")

                    # ダウンロードボタン表示
                    with open(export_file, 'rb') as f:
                        st.download_button(
                            label="📥 ファイルダウンロード",
                            data=f.read(),
                            file_name=Path(export_file).name,
                            mime='application/octet-stream'
                        )
                else:
                    st.error("❌ エクスポートに失敗しました")

            except Exception as e:
                st.error(f"❌ エクスポートエラー: {e}")

    # 3. データ削除・クリーンアップ機能
    st.subheader("🧹 データメンテナンス")

    with st.expander("⚠️ 上級者向け - データクリーンアップ"):
        st.warning(
            "🚨 **注意**: この操作は元に戻せません。"
            "必ず事前にバックアップを取得してください。"
        )

        cleanup_days = st.number_input(
            "削除対象（指定日数より古いデータ）",
            min_value=30,
            max_value=365,
            value=90,
            help="指定した日数より古いデータを削除します"
        )

        if st.button("🗑️ 古いデータ削除実行", type="secondary"):
            confirm = st.checkbox("削除を実行することを理解しました")

            if confirm:
                # クリーンアップ実行
                deleted_count = db.cleanup_old_data(cleanup_days)
                st.success(f"✅ {deleted_count}件のレコードを削除しました")
            else:
                st.warning("確認チェックを入れてください")
```

### 6.4 💡 運用時の重要ポイント

#### 6.4.1 パフォーマンス最適化

```python
# データベースインデックス最適化
PERFORMANCE_INDEXES = """
    -- 高速検索用インデックス
    CREATE INDEX IF NOT EXISTS idx_conversation_session_time
    ON conversation_history(session_id, timestamp);

    CREATE INDEX IF NOT EXISTS idx_conversation_product_time
    ON conversation_history(product_name, timestamp);

    CREATE INDEX IF NOT EXISTS idx_feedback_conversation
    ON user_feedback_detailed(conversation_id);

    CREATE INDEX IF NOT EXISTS idx_sessions_start_time
    ON user_sessions(start_time);
"""
```

#### 6.4.2 セキュリティ・プライバシー対策

```python
# 個人情報保護設定
PRIVACY_CONFIG = {
    "anonymize_ip": True,           # IPアドレスの匿名化
    "hash_user_agents": True,       # User-Agentのハッシュ化
    "retention_period_days": 365,   # データ保持期間
    "auto_delete_old_data": True,   # 古いデータの自動削除
    "encrypt_sensitive_data": True   # 機密データの暗号化
}
```

    def save_chat_message(self, session_id, product_name, user_message,
                         bot_response, sources_used="", prompt_style=""):
        """チャットメッセージを保存"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                INSERT INTO chat_history
                (timestamp, session_id, product_name, user_message,
                 bot_response, sources_used, prompt_style)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            """, (
                datetime.now().isoformat(),
                session_id,
                product_name,
                user_message,
                bot_response,
                sources_used,
                prompt_style
            ))
            conn.commit()

    def get_chat_history(self, product_name, limit=100):
        """チャット履歴を取得"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute("""
                SELECT timestamp, user_message, bot_response, sources_used
                FROM chat_history
                WHERE product_name = ?
                ORDER BY timestamp DESC
                LIMIT ?
            """, (product_name, limit))
            return cursor.fetchall()

    def save_feedback(self, session_id, satisfaction, feedback_text=""):
        """フィードバックを保存"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                INSERT INTO user_feedback (session_id, satisfaction, feedback_text)
                VALUES (?, ?, ?)
            """, (session_id, satisfaction, feedback_text))
            conn.commit()

    def get_statistics(self):
        """利用統計を取得"""
        with sqlite3.connect(self.db_path) as conn:
            # 総チャット数
            total_chats = conn.execute(
                "SELECT COUNT(*) FROM chat_history"
            ).fetchone()[0]

            # 満足度統計
            satisfaction_stats = conn.execute("""
                SELECT satisfaction, COUNT(*)
                FROM user_feedback
                GROUP BY satisfaction
            """).fetchall()

            return {
                "total_chats": total_chats,
                "satisfaction_stats": satisfaction_stats
            }

# グローバルインスタンス
persistent_db = PersistentDatabase()
```

#### 6.1.2 フィードバック管理の更新

`utils/feedback_manager.py`を永続化対応に更新：

```python
# 既存のFeedbackManagerクラスに追加
from config.database import persistent_db

def save_chat_message(self, product_name, user_message, bot_response,
                     sources_used, prompt_style):
    """永続化対応チャットメッセージ保存"""
    session_id = self.get_session_id(product_name)

    # データベースに保存
    persistent_db.save_chat_message(
        session_id=session_id,
        product_name=product_name,
        user_message=user_message,
        bot_response=bot_response,
        sources_used="; ".join(sources_used),
        prompt_style=prompt_style
    )

    # CSVファイルにも保存（互換性のため）
    return super().save_chat_message(
        product_name, user_message, bot_response, sources_used, prompt_style
    )
```

### 6.2 データ容量管理

```python
# config/data_management.py
class DataManager:
    """データ容量管理システム"""

    def __init__(self):
        self.max_chat_records = 10000  # 最大1万件
        self.max_db_size_mb = 50       # データベース最大50MB

    def cleanup_old_chats(self, days_old=90):
        """古いチャット履歴を削除"""
        cutoff_date = datetime.now() - timedelta(days=days_old)

        with sqlite3.connect("data/chatbot.db") as conn:
            conn.execute("""
                DELETE FROM chat_history
                WHERE timestamp < ?
            """, (cutoff_date.isoformat(),))
            conn.commit()

    def get_database_size(self):
        """データベースサイズを取得"""
        db_path = Path("data/chatbot.db")
        if db_path.exists():
            return db_path.stat().st_size / (1024 * 1024)  # MB
        return 0
```

### 3.1 デプロイ手順（5分）

1. **Streamlit Cloud**にアクセス
   ```
   https://share.streamlit.io/
   ```

2. **GitHub連携**
   - "New app" → GitHubリポジトリを選択
   - Main file: `app.py`
   - Branch: `main`

3. **Secrets設定**
   - App settings → Secrets
   - `.streamlit/secrets.toml`の内容をコピー

4. **デプロイ実行**
   - "Deploy!" クリック
   - 自動ビルド開始（2-3分）

### 3.2 カスタムドメイン設定

```bash
# Streamlit Cloud Pro（有料）でカスタムドメイン可能
# 無料版: https://your-app-name.streamlit.app/
```

---

## 4. AWS デプロイ

### 4.1 EC2 デプロイ

**4.1.1 EC2インスタンス作成**
```bash
# Amazon Linux 2
# t3.medium以上推奨（RAG処理のため）
# セキュリティグループ: HTTP(80), HTTPS(443), SSH(22)
```

**4.1.2 環境構築**
```bash
# Python 3.11インストール
sudo yum update -y
sudo yum install -y python3.11 python3.11-pip git

# アプリケーション配置
git clone https://github.com/your-repo/wiki-chatbot.git
cd wiki-chatbot
pip3.11 install -r requirements.txt
```

**4.1.3 環境変数設定**
```bash
# /etc/environment に追加
sudo nano /etc/environment

OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...
GOOGLE_API_KEY=...
WEB_PASSWORD=secure-password
```

**4.1.4 自動起動設定**
```bash
# systemdサービス作成
sudo nano /etc/systemd/system/wiki-chatbot.service

[Unit]
Description=Wiki Chatbot
After=network.target

[Service]
Type=simple
User=ec2-user
WorkingDirectory=/home/ec2-user/wiki-chatbot
ExecStart=/usr/local/bin/streamlit run app.py --server.port=8501 --server.address=0.0.0.0
Restart=always

[Install]
WantedBy=multi-user.target

# サービス有効化
sudo systemctl enable wiki-chatbot
sudo systemctl start wiki-chatbot
```

### 4.2 ECS デプロイ

**Dockerfile**
```dockerfile
FROM python:3.11-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .

EXPOSE 8501

HEALTHCHECK CMD curl --fail http://localhost:8501/_stcore/health

ENTRYPOINT ["streamlit", "run", "app.py", "--server.port=8501", "--server.address=0.0.0.0"]
```

---

## 5. Google Cloud デプロイ

### 5.1 Cloud Run デプロイ

```bash
# 1. プロジェクト設定
gcloud config set project YOUR_PROJECT_ID

# 2. Container Registry有効化
gcloud services enable run.googleapis.com
gcloud services enable containerregistry.googleapis.com

# 3. コンテナビルド
gcloud builds submit --tag gcr.io/YOUR_PROJECT_ID/wiki-chatbot

# 4. Cloud Runデプロイ
gcloud run deploy wiki-chatbot \
  --image gcr.io/YOUR_PROJECT_ID/wiki-chatbot \
  --platform managed \
  --region asia-northeast1 \
  --allow-unauthenticated \
  --set-env-vars OPENAI_API_KEY=sk-... \
  --set-env-vars WEB_PASSWORD=secure-password \
  --memory 2Gi \
  --cpu 1
```

---

## 6. Azure デプロイ

### 6.1 Container Instances

```bash
# 1. リソースグループ作成
az group create --name wiki-chatbot-rg --location japaneast

# 2. Container Registry作成
az acr create --resource-group wiki-chatbot-rg \
  --name wikichatbotacr --sku Basic

# 3. イメージプッシュ
az acr build --registry wikichatbotacr \
  --image wiki-chatbot:latest .

# 4. Container Instance デプロイ
az container create \
  --resource-group wiki-chatbot-rg \
  --name wiki-chatbot-app \
  --image wikichatbotacr.azurecr.io/wiki-chatbot:latest \
  --dns-name-label wiki-chatbot-unique \
  --ports 8501 \
  --environment-variables \
    OPENAI_API_KEY=sk-... \
    WEB_PASSWORD=secure-password
```

---

## 7. セキュリティ設定

### 7.1 認証強化

**パスワード認証**
```python
# config/web_settings.py で設定
WEB_PASSWORD = "complex-password-123"
MAX_LOGIN_ATTEMPTS = 3
```

**セッション管理**
```python
SESSION_TIMEOUT_HOURS = 8  # 8時間でタイムアウト
MAX_QUERIES_PER_SESSION = 100  # セッション毎の上限
```

### 7.2 HTTPS化

**Streamlit Cloud**: 自動でHTTPS

**AWS**: Application Load Balancer + SSL証明書

**GCP**: Cloud Load Balancing + SSL証明書

### 7.3 ファイアウォール設定

```bash
# AWS Security Group
# インバウンド: HTTPS(443), HTTP(80)のみ
# アウトバウンド: HTTPS(443)のみ（API通信用）

# GCP Firewall Rules
gcloud compute firewall-rules create allow-wiki-chatbot \
  --allow tcp:8501 \
  --source-ranges 0.0.0.0/0 \
  --description "Allow Wiki Chatbot access"
```

---

## 8. RAGデータベース移管

### 8.1 ローカルから本番環境へ

**8.1.1 データディレクトリの移管**
```bash
# ローカルのdata/を本番環境にアップロード
# Streamlit Cloud: GitHub経由で自動同期
# AWS: S3バケット経由でアップロード
# GCP: Cloud Storage経由でアップロード
```

**8.1.2 ベクトルストアの再構築**
```bash
# 本番環境で初回起動時に自動実行される
# または管理画面から手動で「インデックス再構築」実行
```

### 8.2 永続化ストレージ設定

**AWS EFS マウント**
```bash
# EFS作成・マウントしてdata/ディレクトリを永続化
sudo mount -t efs fs-12345:/ /home/ec2-user/wiki-chatbot/data/
```

**GCP Persistent Disk**
```yaml
# Cloud Run用の永続化設定
apiVersion: run.googleapis.com/v1
kind: Service
spec:
  template:
    spec:
      volumes:
      - name: data-volume
        csi:
          driver: pd.csi.storage.gke.io
          volumeAttributes:
            type: pd-standard
```

---

## 9. トラブルシューティング

### 9.1 よくある問題

**❌ メモリ不足エラー**
```bash
# 解決策: インスタンスサイズアップ
# AWS: t3.medium → t3.large
# GCP: --memory 2Gi → --memory 4Gi
```

**❌ API Key エラー**
```bash
# 解決策: 環境変数の確認
# Streamlit Cloud: Secrets設定を再確認
# AWS/GCP: 環境変数の設定を確認
```

**❌ ファイルアップロードエラー**
```bash
# 解決策: アップロード制限の調整
# streamlit config set server.maxUploadSize 200
```

### 9.2 パフォーマンス最適化

**キャッシュ設定**
```python
# config/settings.py
ENABLE_CACHE = True
CACHE_TTL = 3600  # 1時間
```

**同時接続数制限**
```python
# config/web_settings.py
MAX_CONCURRENT_USERS = 10
```

### 9.3 ログ・モニタリング

**基本ログ設定**
```python
# utils/logger.py
LOG_LEVEL = "INFO"
LOG_FORMAT = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
```

**外部モニタリング**
- AWS: CloudWatch
- GCP: Cloud Monitoring
- Azure: Application Insights

---

## 🚀 デプロイメント完了チェックリスト

### 事前準備
- [ ] API Key取得・設定完了
- [ ] GitHubリポジトリpush完了
- [ ] requirements.txt更新完了

### セキュリティ
- [ ] WEB_PASSWORD設定完了
- [ ] HTTPS有効化完了
- [ ] ファイアウォール設定完了

### 機能テスト
- [ ] ログイン機能動作確認
- [ ] チャット機能動作確認
- [ ] ファイルアップロード動作確認
- [ ] 管理画面アクセス確認

### 運用準備
- [ ] モニタリング設定完了
- [ ] バックアップ設定完了
- [ ] アクセスログ確認方法整備完了

---

**最終更新**: 2025年9月15日
**対応バージョン**: Wiki Chatbot v4.0
**推奨デプロイ**: Streamlit Cloud（小規模）、AWS/GCP（本格運用）