# çµ±åˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ« - å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä¸€æ‹¬ç®¡ç†

import os
from dataclasses import dataclass
from typing import Dict, Any, List, Optional
import streamlit as st


@dataclass
class LLMModelConfig:
    """LLMãƒ¢ãƒ‡ãƒ«è¨­å®š"""

    name: str
    max_tokens: int
    temperature: float
    top_p: float
    frequency_penalty: float
    presence_penalty: float
    cost_per_1k_tokens_input: float
    cost_per_1k_tokens_output: float
    context_window: int
    supports_function_calling: bool = False


@dataclass
class RAGConfig:
    """RAGå‡¦ç†è¨­å®š"""

    chunk_size: int
    chunk_overlap: int
    search_top_k: int
    embedding_model: str
    vector_store_type: str
    similarity_threshold: float


@dataclass
class SystemConfig:
    """ã‚·ã‚¹ãƒ†ãƒ è¨­å®š"""

    app_title: str
    app_icon: str
    max_file_size_mb: int
    supported_file_types: List[str]
    session_timeout_minutes: int
    log_level: str
    enable_debug_mode: bool


class Settings:
    """çµ±åˆè¨­å®šç®¡ç†ã‚¯ãƒ©ã‚¹"""

    # === LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åˆ¥è¨­å®š ===
    LLM_PROVIDERS = {
        "openai": {
            "name": "OpenAI",
            "api_key_env": "OPENAI_API_KEY",
            "models": {
                "gpt-3.5-turbo": LLMModelConfig(
                    name="GPT-3.5 Turbo",
                    max_tokens=4000,
                    temperature=0.3,
                    top_p=0.9,
                    frequency_penalty=0.0,
                    presence_penalty=0.0,
                    cost_per_1k_tokens_input=0.0015,
                    cost_per_1k_tokens_output=0.002,
                    context_window=16385,
                    supports_function_calling=True,
                ),
                "gpt-4": LLMModelConfig(
                    name="GPT-4",
                    max_tokens=8000,
                    temperature=0.2,
                    top_p=0.9,
                    frequency_penalty=0.0,
                    presence_penalty=0.0,
                    cost_per_1k_tokens_input=0.03,
                    cost_per_1k_tokens_output=0.06,
                    context_window=8192,
                    supports_function_calling=True,
                ),
                "gpt-4o": LLMModelConfig(
                    name="GPT-4o",
                    max_tokens=4000,
                    temperature=0.2,
                    top_p=0.9,
                    frequency_penalty=0.0,
                    presence_penalty=0.0,
                    cost_per_1k_tokens_input=0.005,
                    cost_per_1k_tokens_output=0.015,
                    context_window=128000,
                    supports_function_calling=True,
                ),
            },
        },
        "anthropic": {
            "name": "Anthropic (Claude)",
            "api_key_env": "ANTHROPIC_API_KEY",
            "models": {
                "claude-3-haiku": LLMModelConfig(
                    name="Claude 3 Haiku",
                    max_tokens=4000,
                    temperature=0.3,
                    top_p=0.9,
                    frequency_penalty=0.0,
                    presence_penalty=0.0,
                    cost_per_1k_tokens_input=0.00025,
                    cost_per_1k_tokens_output=0.00125,
                    context_window=200000,
                ),
                "claude-3-sonnet": LLMModelConfig(
                    name="Claude 3 Sonnet",
                    max_tokens=4000,
                    temperature=0.2,
                    top_p=0.9,
                    frequency_penalty=0.0,
                    presence_penalty=0.0,
                    cost_per_1k_tokens_input=0.003,
                    cost_per_1k_tokens_output=0.015,
                    context_window=200000,
                ),
                "claude-3-opus": LLMModelConfig(
                    name="Claude 3 Opus",
                    max_tokens=4000,
                    temperature=0.1,
                    top_p=0.9,
                    frequency_penalty=0.0,
                    presence_penalty=0.0,
                    cost_per_1k_tokens_input=0.015,
                    cost_per_1k_tokens_output=0.075,
                    context_window=200000,
                ),
            },
        },
        "google": {
            "name": "Google (Gemini)",
            "api_key_env": "GOOGLE_API_KEY",
            "models": {
                "gemini-1.5-flash": LLMModelConfig(
                    name="Gemini 1.5 Flash",
                    max_tokens=8192,
                    temperature=0.3,
                    top_p=0.95,
                    frequency_penalty=0.0,
                    presence_penalty=0.0,
                    cost_per_1k_tokens_input=0.000075,
                    cost_per_1k_tokens_output=0.0003,
                    context_window=1000000,
                ),
                "gemini-1.5-pro": LLMModelConfig(
                    name="Gemini 1.5 Pro",
                    max_tokens=8192,
                    temperature=0.2,
                    top_p=0.95,
                    frequency_penalty=0.0,
                    presence_penalty=0.0,
                    cost_per_1k_tokens_input=0.00125,
                    cost_per_1k_tokens_output=0.00375,
                    context_window=2000000,
                ),
            },
        },
    }

    # === RAGè¨­å®š ===
    RAG_SETTINGS = {
        "general": RAGConfig(
            chunk_size=1000,
            chunk_overlap=200,
            search_top_k=5,
            embedding_model="text-embedding-ada-002",
            vector_store_type="chromadb",
            similarity_threshold=0.7,
        ),
        "short_docs": RAGConfig(
            chunk_size=500,
            chunk_overlap=100,
            search_top_k=7,
            embedding_model="text-embedding-ada-002",
            vector_store_type="chromadb",
            similarity_threshold=0.75,
        ),
        "long_docs": RAGConfig(
            chunk_size=1500,
            chunk_overlap=300,
            search_top_k=3,
            embedding_model="text-embedding-ada-002",
            vector_store_type="chromadb",
            similarity_threshold=0.65,
        ),
        "technical": RAGConfig(
            chunk_size=2000,
            chunk_overlap=400,
            search_top_k=4,
            embedding_model="text-embedding-ada-002",
            vector_store_type="chromadb",
            similarity_threshold=0.6,
        ),
    }

    # === ã‚·ã‚¹ãƒ†ãƒ è¨­å®š ===
    SYSTEM_CONFIG = SystemConfig(
        app_title="ç¤¾å†…Wikiæ¤œç´¢ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ",
        app_icon="ğŸ’¬",
        max_file_size_mb=50,
        supported_file_types=["txt", "pdf", "docx", "html", "csv", "xlsx", "pptx", "json", "md"],
        session_timeout_minutes=60,
        log_level="INFO",
        enable_debug_mode=False,
    )

    # === ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼ˆå»ƒæ­¢äºˆå®šï¼‰ ===
    # æ³¨æ„: æ–°ã—ã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚·ã‚¹ãƒ†ãƒ ï¼ˆprompt_manager.pyï¼‰ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„
    SYSTEM_PROMPTS = {
        # äº’æ›æ€§ã®ãŸã‚ã«æ®‹ã—ã¦ã„ã¾ã™ãŒã€æ–°è¦å®Ÿè£…ã§ã¯ prompt_manager ã‚’ä½¿ç”¨
    }

    # === ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šå–å¾—ãƒ¡ã‚½ãƒƒãƒ‰ ===
    @classmethod
    def get_default_provider(cls) -> str:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’å–å¾—ï¼ˆå„ªå…ˆé †ä½: ChatGPT -> Anthropic -> Geminiï¼‰"""
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«ä¿å­˜ã•ã‚ŒãŸé¸æŠãŒã‚ã‚Œã°å„ªå…ˆ
        if "selected_provider" in st.session_state:
            return st.session_state["selected_provider"]

        # å„ªå…ˆé †ä½ã«å¾“ã£ã¦åˆ©ç”¨å¯èƒ½ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’é¸æŠ
        priority_order = ["openai", "anthropic", "google"]

        for provider in priority_order:
            if cls.get_api_key(provider):
                return provider

        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆAPI Keyæœªè¨­å®šã§ã‚‚æœ€åˆã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’è¿”ã™ï¼‰
        return "openai"

    @classmethod
    def get_default_model(cls, provider: str) -> str:
        """æŒ‡å®šãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—"""
        models = list(cls.LLM_PROVIDERS[provider]["models"].keys())
        session_key = f"selected_model_{provider}"

        # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‹ã‚‰å–å¾—
        saved_model = st.session_state.get(session_key)

        # ä¿å­˜ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ãŒç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã«å­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        if saved_model and saved_model in models:
            return saved_model
        else:
            # å­˜åœ¨ã—ãªã„å ´åˆã¯æœ€åˆã®ãƒ¢ãƒ‡ãƒ«ã‚’è¿”ã—ã€ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’æ›´æ–°
            default_model = models[0]
            st.session_state[session_key] = default_model
            return default_model

    @classmethod
    def get_default_rag_config(cls) -> str:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®RAGè¨­å®šã‚’å–å¾—"""
        return st.session_state.get("selected_rag_config", "general")

    @classmethod
    def get_default_prompt_style(cls) -> str:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¹ã‚¿ã‚¤ãƒ«ã‚’å–å¾—"""
        try:
            # å¾ªç’°ã‚¤ãƒ³ãƒãƒ¼ãƒˆå›é¿ã®ãŸã‚ã€å¿…è¦æ™‚ã®ã¿ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
            from utils.prompt_manager import prompt_manager

            available_prompts = prompt_manager.get_available_prompts()

            # ã¾ãšæ±ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹ã‚‰'general'ã‚’æ¢ã™
            if "generic" in available_prompts and "general" in available_prompts["generic"]:
                return "general"

            # 'general'ãŒãªã„å ´åˆã¯æœ€åˆã®æ±ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨
            if "generic" in available_prompts and available_prompts["generic"]:
                return available_prompts["generic"][0]
        except Exception:
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å®‰å…¨ãªãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¿”ã™
            pass

        # ä½•ã‚‚ãªã„å ´åˆã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        return "general"

    @classmethod
    def get_company_name(cls) -> str:
        """ä¼šç¤¾åã‚’å–å¾—"""
        return st.session_state.get("company_name", "ã‚µãƒ³ãƒ—ãƒ«æ ªå¼ä¼šç¤¾")

    @classmethod
    def get_api_key(cls, provider: str) -> Optional[str]:
        """æŒ‡å®šãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®API Keyã‚’å–å¾—"""
        env_var = cls.LLM_PROVIDERS[provider]["api_key_env"]

        # 1. Streamlit Secretsã‹ã‚‰å–å¾—
        try:
            return st.secrets.get(env_var)
        except:
            pass

        # 2. ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—
        return os.getenv(env_var)

    @classmethod
    def get_model_config(cls, provider: str, model: str) -> LLMModelConfig:
        """æŒ‡å®šãƒ¢ãƒ‡ãƒ«ã®è¨­å®šã‚’å–å¾—"""
        return cls.LLM_PROVIDERS[provider]["models"][model]

    @classmethod
    def get_rag_config(cls, config_name: str) -> RAGConfig:
        """æŒ‡å®šRAGè¨­å®šã‚’å–å¾—"""
        return cls.RAG_SETTINGS[config_name]

    @classmethod
    def get_system_config(cls) -> SystemConfig:
        """ã‚·ã‚¹ãƒ†ãƒ è¨­å®šã‚’å–å¾—"""
        return cls.SYSTEM_CONFIG

    @classmethod
    def get_system_prompt(cls, style: str, **kwargs) -> str:
        """ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å–å¾—ï¼ˆäº’æ›æ€§ã®ãŸã‚æ®‹å­˜ã€æ–°è¦å®Ÿè£…ã¯prompt_managerã‚’ä½¿ç”¨ï¼‰"""
        # äº’æ›æ€§ã®ãŸã‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        from utils.prompt_manager import prompt_manager

        return prompt_manager.get_system_prompt(style, **kwargs)

    @classmethod
    def calculate_cost(cls, provider: str, model: str, input_tokens: int, output_tokens: int) -> float:
        """APIä½¿ç”¨æ–™é‡‘ã‚’è¨ˆç®—"""
        model_config = cls.get_model_config(provider, model)
        input_cost = (input_tokens / 1000) * model_config.cost_per_1k_tokens_input
        output_cost = (output_tokens / 1000) * model_config.cost_per_1k_tokens_output
        return input_cost + output_cost

    @classmethod
    def get_available_providers(cls) -> Dict[str, str]:
        """åˆ©ç”¨å¯èƒ½ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’å–å¾—ï¼ˆAPI Keyè¨­å®šæ¸ˆã¿ã®ã‚‚ã®ã€å„ªå…ˆé †ä½é †ï¼‰"""
        # å„ªå…ˆé †ä½ã«å¾“ã£ã¦é †åºä»˜ã‘
        priority_order = ["openai", "anthropic", "google"]
        available = {}

        # å„ªå…ˆé †ä½é †ã«è¿½åŠ 
        for provider_id in priority_order:
            if provider_id in cls.LLM_PROVIDERS and cls.get_api_key(provider_id):
                available[provider_id] = cls.LLM_PROVIDERS[provider_id]["name"]

        # å„ªå…ˆé †ä½ã«ãªã„ä»–ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚‚è¿½åŠ 
        for provider_id, provider_info in cls.LLM_PROVIDERS.items():
            if provider_id not in priority_order and cls.get_api_key(provider_id):
                available[provider_id] = provider_info["name"]

        return available

    @classmethod
    def validate_settings(cls) -> Dict[str, List[str]]:
        """è¨­å®šã®å¦¥å½“æ€§ã‚’æ¤œè¨¼"""
        issues = {"errors": [], "warnings": []}

        # API Keyç¢ºèª
        for provider_id, provider_info in cls.LLM_PROVIDERS.items():
            api_key = cls.get_api_key(provider_id)
            if not api_key:
                issues["warnings"].append(f"{provider_info['name']} API Key ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºç¢ºèª
        if cls.SYSTEM_CONFIG.max_file_size_mb > 100:
            issues["warnings"].append("ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºåˆ¶é™ãŒ100MBã‚’è¶…ãˆã¦ã„ã¾ã™")

        # ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºç¢ºèª
        for config_name, config in cls.RAG_SETTINGS.items():
            if config.chunk_overlap >= config.chunk_size:
                issues["errors"].append(f"RAGè¨­å®š '{config_name}': chunk_overlapãŒchunk_sizeä»¥ä¸Šã«ãªã£ã¦ã„ã¾ã™")

        return issues


# è¨­å®šã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
settings = Settings()


# ä¾¿åˆ©ãªé–¢æ•°ç¾¤
def get_current_llm_config():
    """ç¾åœ¨é¸æŠã•ã‚Œã¦ã„ã‚‹LLMè¨­å®šã‚’å–å¾—"""
    provider = settings.get_default_provider()
    model = settings.get_default_model(provider)
    return provider, model, settings.get_model_config(provider, model)


def get_current_rag_config():
    """ç¾åœ¨é¸æŠã•ã‚Œã¦ã„ã‚‹RAGè¨­å®šã‚’å–å¾—"""
    config_name = settings.get_default_rag_config()
    return config_name, settings.get_rag_config(config_name)


def update_session_settings(**kwargs):
    """ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨­å®šã‚’æ›´æ–°"""
    for key, value in kwargs.items():
        st.session_state[key] = value
